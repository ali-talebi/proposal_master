{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNyMQMsR3v6RATXoAJjmMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ali-talebi/proposal_master/blob/main/Simulation_1403_07_02_End.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOVuK-BalIRt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import matplotlib.animation as animation\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "MQ6_SnbRlNV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/drive/MyDrive/Malek_Master/code/Total_Data_Simulation'\n"
      ],
      "metadata": {
        "id": "jxNaLUSLlNc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_file_link = []\n",
        "for i in os.listdir(root_path) :\n",
        "  total_file_link.append(root_path+f'/{i}')"
      ],
      "metadata": {
        "id": "6j2yHg1UlNfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_alpha_content = [ i / 1000 for i in range( 1 , 999 ) ]\n",
        "total_flat_content  = [ i for i in range(1 , 10  ) ]\n",
        "total_location      = list(range( 5 , 16))"
      ],
      "metadata": {
        "id": "AJgnVt1IlNhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "points = []\n",
        "start_x = 65\n",
        "start_y = [0 , -5 , -10.0 ]\n",
        "for i in range(15) :\n",
        "  start_x +=  15\n",
        "\n",
        "  for j in range(3) :\n",
        "    points.append([start_x , start_y[j] , 0 ] )\n",
        "\n",
        "points.append([330 , -2.5 , 0 ])\n",
        "points.append([330 , -7.5 , 0 ])\n",
        "points.append([330 , -12.5 , 0 ])\n",
        "total_points = np.array(points)\n",
        "plt.scatter(total_points[ : ,  0 ] , total_points[ : , 1 ] , label = \"target\" )\n",
        "plt.legend()\n",
        "plt.title(\"Blade Shape - Picture 1.7 page 7 document volume 2 \")\n",
        "plt.xlabel(\" X (in) \")\n",
        "plt.ylabel(\" Y (in) \")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FSl8v1TElNkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_simulation = {\n",
        "    'bias_correlation' : [] ,\n",
        "    'betha_0_correlation' : [] ,\n",
        "    'betha_1_correlation' : [] ,\n",
        "    'betha_2_correlation' : [] ,\n",
        "    'betha_3_correlation' : [] ,\n",
        "    'betha_4_correlation' : [] ,\n",
        "    'betha_5_correlation' : [] ,\n",
        "    'alpha' : [] ,\n",
        "    'flap'  : [] ,\n",
        "    'accuracy_nav' : [] ,\n",
        "    'accuracy_random_forest' : [] ,\n",
        "    'accuracy_extra_classifier' : []\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "Z0sPmj2QlNm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter_alpha in total_alpha_content :\n",
        "  for iter_flat in total_flat_content :\n",
        "    print(f\"iter_alpha : {iter_alpha} | iter_flat : {iter_flat} \")\n",
        "    params_health  = []\n",
        "    params_fault   = []\n",
        "    params_fault_t = []\n",
        "    for iter_link in total_file_link :\n",
        "      df_table_change     = pd.read_csv(iter_link)\n",
        "      for locate in total_location:\n",
        "        related_x = -1\n",
        "        related_y = -1\n",
        "        related_z = -1\n",
        "        for iteration in range(1):\n",
        "          alpha = 0\n",
        "          term  = 0\n",
        "          Flat_Add = 0\n",
        "          noise_activate = ( (np.random.randn() + 1 ) / 100 )\n",
        "          #print( f'Alpha : {iter_alpha} ' ,  f'Table Change : {iter_link}' , ' -- ' , f'Locate : {locate} ' , ' -- ' , f\"Iter : {iteration}\" , ' -- ' , '')\n",
        "          select_location = 0\n",
        "\n",
        "          total_health_iter  = []\n",
        "          total_fault_iter   = []\n",
        "          total_fault_t_iter = []\n",
        "          new_data_simulated_ = []\n",
        "          new_data_simulated_fault = []\n",
        "          new_data_simulated_fault_t = []\n",
        "\n",
        "\n",
        "          for i in range(df_table_change.shape[0]):\n",
        "            x_mean = df_table_change.iloc[i, 1]\n",
        "            x_std  = df_table_change.iloc[i, 2]\n",
        "            y_mean = df_table_change.iloc[i, 3]\n",
        "            y_std  = df_table_change.iloc[i, 4]\n",
        "            z_mean = df_table_change.iloc[i, 5]\n",
        "            z_std  = df_table_change.iloc[i, 6]\n",
        "\n",
        "            if i > locate :\n",
        "              alpha = iter_alpha\n",
        "              term  = -1 * ( alpha * abs( i - locate ) + noise_activate )\n",
        "              Flat_Add  = iter_flat / 10\n",
        "\n",
        "            for element in total_points[i * 3: (i + 1) * 3, :]:\n",
        "              x_sample = element[0]\n",
        "              y_sample = element[1]\n",
        "              z_sample = element[2]\n",
        "              rng = np.random.default_rng()\n",
        "              x_added  = rng.normal(x_mean, x_std, size=1)\n",
        "              x_added  = x_added.tolist()[0]\n",
        "              x_sample += related_x * x_added\n",
        "\n",
        "              y_added  = rng.normal(y_mean, y_std, size=1)\n",
        "              y_added  = y_added.tolist()[0]\n",
        "              y_sample += related_y * y_added\n",
        "\n",
        "              z_added  = rng.normal(z_mean, z_std, size=1)\n",
        "              z_base   = z_sample\n",
        "\n",
        "              z_added_term  = z_added.tolist()[0] + term\n",
        "              z_added       = z_added.tolist()[0]\n",
        "              z_sample_new_term = z_base + related_z * z_added_term\n",
        "              z_sample_new      = z_base + related_z * z_added\n",
        "              z_flat  = z_sample_new_term\n",
        "              if select_location == 0 :\n",
        "                if np.random.randint(0 ,2 ) :\n",
        "                  z_flat = z_sample_new_term + Flat_Add\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              new_data_simulated_.append([ x_sample, y_sample, z_sample_new  ])\n",
        "              new_data_simulated_fault.append([ x_sample, y_sample, z_sample_new_term  ])\n",
        "              new_data_simulated_fault_t.append([ x_sample, y_sample, z_flat  ])\n",
        "\n",
        "              total_health_iter.append([ x_sample, y_sample, z_sample_new  ])\n",
        "              total_fault_iter.append([ x_sample, y_sample, z_sample_new_term  ])\n",
        "              total_fault_t_iter.append([ x_sample, y_sample, z_flat  ])\n",
        "\n",
        "          fig = plt.figure(figsize=(30 , 10 ))\n",
        "          ax0 = fig.add_subplot(1 , 6 , 1 , projection='3d' )\n",
        "          ax1 = fig.add_subplot(1 , 6 , 2 , projection='3d' )\n",
        "          ax2 = fig.add_subplot(1 , 6 , 3 , projection='3d' )\n",
        "          ax3 = fig.add_subplot(1 , 6 , 4 )\n",
        "          ax4 = fig.add_subplot(1 , 6 , 5 )\n",
        "          ax5 = fig.add_subplot(1 , 6 , 6 )\n",
        "\n",
        "          total_health_iter = np.array(total_health_iter)\n",
        "          total_health_iter = np.array(total_health_iter)\n",
        "          total_fault_iter = np.array(total_fault_iter)\n",
        "          total_fault_t_iter = np.array(total_fault_t_iter)\n",
        "          print(f\"iter_alpha : {iter_alpha} , flap: {iter_flat} , locate : {locate} ,  iteration : {iteration} \")\n",
        "          ax0.scatter3D(total_health_iter[: , 0 ] , total_health_iter[: , 1 ] , total_health_iter[: , 2 ]  , label='H' , c='r' , s = 30   )\n",
        "          ax0.scatter3D(total_fault_iter[: , 0 ] , total_fault_iter[: , 1 ] , total_fault_iter[: , 2 ]  , label='F1' , c ='b'  , s = 30  )\n",
        "          ax1.scatter3D(total_fault_iter[: , 0 ] , total_fault_iter[: , 1 ] , total_fault_iter[: , 2 ]  ,       label='F1' , c ='b', s = 30    )\n",
        "          ax1.scatter3D(total_fault_t_iter[: , 0 ] , total_fault_t_iter[: , 1 ] , total_fault_t_iter[: , 2 ]  , label='F2' , c='r' , s = 30 )\n",
        "          ax2.scatter3D(total_fault_t_iter[: , 0 ] , total_fault_t_iter[: , 1 ] , total_fault_t_iter[: , 2 ]  , label='F2' , c='r'  , s = 30  )\n",
        "\n",
        "          ax3.scatter(total_health_iter[: , 0 ]  , total_health_iter[: , 2 ]  ,       label='H' , c ='b' )\n",
        "          ax3.scatter(total_fault_iter[: , 0 ]  , total_fault_iter[: , 2 ]  ,       label='F1' , c ='r' )\n",
        "\n",
        "          ax4.scatter(total_health_iter[: , 0 ]  , total_health_iter[: , 2 ]  ,       label='H' , c ='b' )\n",
        "          ax4.scatter(total_fault_t_iter[: , 0 ]  , total_fault_t_iter[: , 2 ]  , label='F2' , c='r')\n",
        "\n",
        "          ax5.scatter(total_fault_iter[: , 0 ]  , total_fault_iter[: , 2 ]  ,       label='F1' , c ='b' )\n",
        "          ax5.scatter(total_fault_t_iter[: , 0 ]  , total_fault_t_iter[: , 2 ]  , label='F2' , c='r')\n",
        "\n",
        "          ax0.legend()\n",
        "          ax0.grid()\n",
        "          ax1.legend()\n",
        "          ax1.grid()\n",
        "          ax2.legend()\n",
        "          ax2.grid()\n",
        "          ax3.legend()\n",
        "          ax3.grid()\n",
        "\n",
        "          ax4.legend()\n",
        "          ax4.grid()\n",
        "          ax5.legend()\n",
        "          ax5.grid()\n",
        "\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          new_data_simulated_        = np.array(new_data_simulated_)\n",
        "          new_data_simulated_fault   = np.array(new_data_simulated_fault)\n",
        "          new_data_simulated_fault_t = np.array(new_data_simulated_fault_t)\n",
        "\n",
        "\n",
        "          #fig = plt.figure(figsize=(30, 10))\n",
        "          df_health = pd.DataFrame()\n",
        "          df_health['X_'] = new_data_simulated_[:, 0]\n",
        "          df_health['Y_'] = new_data_simulated_[:, 1]\n",
        "          df_health['Z_'] = new_data_simulated_[:, 2]\n",
        "\n",
        "          poly = PolynomialFeatures(degree=2)\n",
        "          x_poly = poly.fit_transform(df_health[['X_', 'Y_']])\n",
        "          scaler = StandardScaler()\n",
        "          df_stander = pd.DataFrame(scaler.fit_transform(x_poly), columns=['0', 'X', 'Y', 'x1^2', 'x1x2', 'x2^2', ])\n",
        "          df_stander['Z_'] = df_health['Z_']\n",
        "          x_train, x_test, z_train, z_test = train_test_split(df_stander[['0', 'X', 'Y', 'x1^2', 'x1x2', 'x2^2']],\n",
        "                                                                          df_stander[\"Z_\"])\n",
        "          model = LinearRegression()\n",
        "          model.fit(x_train, z_train)\n",
        "          z_predict = model.predict(x_test)\n",
        "          intercept_0 = model.intercept_\n",
        "          coef1 = model.coef_[0]\n",
        "          coef2 = model.coef_[1]\n",
        "          coef3 = model.coef_[2]\n",
        "          coef4 = model.coef_[3]\n",
        "          coef5 = model.coef_[4]\n",
        "          coef6 = model.coef_[5]\n",
        "\n",
        "          total_new_generate = []\n",
        "          total_error = []\n",
        "          for i in range(len(df_health['X_'])):\n",
        "            new_value = intercept_0 + coef1 * df_stander.iloc[i, 0] + coef2 * df_stander.iloc[i, 1] + coef3 * df_stander.iloc[i, 2] + coef4 * df_stander.iloc[i, 3] + coef5 * df_stander.iloc[i, 4]\n",
        "            + coef6 * df_stander.iloc[i, 5]\n",
        "            total_new_generate.append(new_value)\n",
        "            error = df_stander.iloc[i, -1] - new_value\n",
        "            total_error.append(error)\n",
        "\n",
        "\n",
        "          #class health == 0\n",
        "          params_health.append([intercept_0, coef1, coef2, coef3, coef4, coef5, coef6, mean_squared_error(z_predict, z_test),locate , 0 , iter_alpha ,  iter_flat  ])\n",
        "          total_change = [1, 1]\n",
        "          related_x *= total_change[np.random.randint(0, 2)]\n",
        "          related_y *= total_change[np.random.randint(0, 2)]\n",
        "          related_z *= total_change[np.random.randint(0, 2)]\n",
        "          # ---- setup for class Fault ----\n",
        "          df_fault = pd.DataFrame()\n",
        "          df_fault['X_'] = new_data_simulated_fault[:, 0]\n",
        "          df_fault['Y_'] = new_data_simulated_fault[:, 1]\n",
        "          df_fault['Z_'] = new_data_simulated_fault[:, 2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          poly_fault = PolynomialFeatures(degree=2)\n",
        "          x_poly_fault = poly_fault.fit_transform(df_fault[['X_', 'Y_']])\n",
        "          scaler_fault = StandardScaler()\n",
        "          df_stander_fault = pd.DataFrame(scaler_fault.fit_transform(x_poly_fault), columns=['0', 'X', 'Y', 'x1^2', 'x1x2', 'x2^2', ])\n",
        "          df_stander_fault['Z_'] = df_fault['Z_']\n",
        "          x_train_fault , x_test_fault , z_train_fault , z_test_fault = train_test_split(df_stander_fault[['0', 'X', 'Y', 'x1^2', 'x1x2', 'x2^2']], df_stander_fault[\"Z_\"])\n",
        "          model_fault = LinearRegression()\n",
        "          model_fault.fit(x_train_fault, z_train_fault)\n",
        "          z_predict_fault = model_fault.predict(x_test_fault)\n",
        "          intercept_0 = model_fault.intercept_\n",
        "          coef1 = model_fault.coef_[0]\n",
        "          coef2 = model_fault.coef_[1]\n",
        "          coef3 = model_fault.coef_[2]\n",
        "          coef4 = model_fault.coef_[3]\n",
        "          coef5 = model_fault.coef_[4]\n",
        "          coef6 = model_fault.coef_[5]\n",
        "          total_new_generate_fault = []\n",
        "          total_error_fault = []\n",
        "          for i in range(len(df_fault['X_'])):\n",
        "            new_value = intercept_0 + coef1 * df_stander_fault.iloc[i, 0] + coef2 * df_stander_fault.iloc[i, 1] + coef3 *  df_stander_fault.iloc[i, 2] + coef4 * df_stander_fault.iloc[i, 3] + coef5 * df_stander_fault.iloc[i, 4]  + coef6 * df_stander_fault.iloc[i, 5]\n",
        "            total_new_generate_fault.append(new_value)\n",
        "            error_Fault = df_stander_fault.iloc[i, -1] - new_value\n",
        "            total_error_fault.append(error_Fault)\n",
        "\n",
        "          params_fault.append([intercept_0, coef1, coef2, coef3, coef4, coef5, coef6, mean_squared_error(z_predict_fault, z_test_fault),locate ,  1 , iter_alpha ,  iter_flat ])\n",
        "          total_change = [1, 1]\n",
        "          related_x *= total_change[np.random.randint(0, 2)]\n",
        "          related_y *= total_change[np.random.randint(0, 2)]\n",
        "          related_z *= total_change[np.random.randint(0, 2)]\n",
        "\n",
        "          # ---- setup for class Fault T ----\n",
        "\n",
        "          df_fault_t = pd.DataFrame()\n",
        "          df_fault_t['X_'] = new_data_simulated_fault_t[:, 0]\n",
        "          df_fault_t['Y_'] = new_data_simulated_fault_t[:, 1]\n",
        "          df_fault_t['Z_'] = new_data_simulated_fault_t[:, 2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          poly_fault_t = PolynomialFeatures(degree=2)\n",
        "          x_poly_fault_t = poly_fault_t.fit_transform(df_fault_t[['X_', 'Y_']])\n",
        "          scaler_fault_t = StandardScaler()\n",
        "          df_stander_fault_t = pd.DataFrame(scaler_fault_t.fit_transform(x_poly_fault_t), columns=['0', 'X', 'Y', 'x1^2', 'x1x2', 'x2^2', ])\n",
        "          df_stander_fault_t['Z_'] = df_fault_t['Z_']\n",
        "          x_train_fault_t , x_test_fault_t , z_train_fault_t , z_test_fault_t = train_test_split(df_stander_fault_t[['0', 'X', 'Y', 'x1^2', 'x1x2', 'x2^2']], df_stander_fault[\"Z_\"])\n",
        "          model_fault_t = LinearRegression()\n",
        "          model_fault_t.fit(x_train_fault_t, z_train_fault_t)\n",
        "          z_predict_fault = model_fault_t.predict(x_test_fault_t)\n",
        "          intercept_0 = model_fault_t.intercept_\n",
        "          coef1 = model_fault_t.coef_[0]\n",
        "          coef2 = model_fault_t.coef_[1]\n",
        "          coef3 = model_fault_t.coef_[2]\n",
        "          coef4 = model_fault_t.coef_[3]\n",
        "          coef5 = model_fault_t.coef_[4]\n",
        "          coef6 = model_fault_t.coef_[5]\n",
        "          total_new_generate_fault_t = []\n",
        "          total_error_fault_t = []\n",
        "          for i in range(len(df_fault['X_'])):\n",
        "            new_value = intercept_0 + coef1 * df_stander_fault_t.iloc[i, 0] + coef2 * df_stander_fault_t.iloc[i, 1] + coef3 *  df_stander_fault_t.iloc[i, 2] + coef4 * df_stander_fault_t.iloc[i, 3] + coef5 * df_stander_fault_t.iloc[i, 4]  + coef6 * df_stander_fault_t.iloc[i, 5]\n",
        "            total_new_generate_fault_t.append(new_value)\n",
        "            error_Fault = df_stander_fault_t.iloc[i, -1] - new_value\n",
        "            total_error_fault.append(error_Fault)\n",
        "\n",
        "          params_fault_t.append([intercept_0, coef1, coef2, coef3, coef4, coef5, coef6, mean_squared_error(z_predict_fault, z_test_fault),locate ,  2 , iter_alpha ,  iter_flat ])\n",
        "          total_change = [1, 1]\n",
        "          related_x *= total_change[np.random.randint(0, 2)]\n",
        "          related_y *= total_change[np.random.randint(0, 2)]\n",
        "          related_z *= total_change[np.random.randint(0, 2)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#----\n",
        "    params_health = np.array(params_health)\n",
        "    df_params = pd.DataFrame({'bias':params_health[: , 0 ] , 'betha_0':params_health[ : , 1 ] , 'betha_1':params_health[ : , 2 ] , 'betha_2':params_health[ : , 3 ] ,  'betha_3':params_health[ : , 4 ] , 'betha_4':params_health[ : , 5 ] , 'betha_5':params_health[ : , 6 ] , 'error' : params_health[ : , 7 ]  ,\n",
        "                          'locate' : params_health[: , 8 ].astype(int)   , 'class' :params_health[: , 9 ] ,\n",
        "                              'alpha' :params_health[: , 10 ] , 'flap' : params_health[: , 11 ] } )\n",
        "\n",
        "\n",
        "\n",
        "    params_fault = np.array(params_fault)\n",
        "    df_params_fault = pd.DataFrame({'bias':params_fault[: , 0 ] , 'betha_0':params_fault[ : , 1 ] , 'betha_1':params_fault[ : , 2 ] , 'betha_2':params_fault[ : , 3 ] ,  'betha_3':params_fault[ : , 4 ] , 'betha_4':params_fault[ : , 5 ] , 'betha_5':params_fault[ : , 6 ] , 'error' : params_fault[ : , 7 ]  ,\n",
        "                            'locate' : params_fault[: , 8 ].astype(int) ,\n",
        "                                    'class' :params_fault[: , 9 ]  ,\n",
        "                                    'alpha' :params_fault[: , 10 ] , 'flap' : params_fault[: , 11 ] }   )\n",
        "\n",
        "\n",
        "    params_fault_t = np.array(params_fault_t)\n",
        "    df_params_fault_t = pd.DataFrame({'bias':params_fault_t[: , 0 ] , 'betha_0':params_fault_t[ : , 1 ] , 'betha_1':params_fault_t[ : , 2 ] , 'betha_2':params_fault_t[ : , 3 ] ,  'betha_3':params_fault_t[ : , 4 ] , 'betha_4':params_fault_t[ : , 5 ] , 'betha_5':params_fault_t[ : , 6 ] , 'error' : params_fault_t[ : , 7 ]  ,\n",
        "                            'locate' : params_fault_t[: , 8 ].astype(int) ,\n",
        "                                    'class' :params_fault_t[: , 9 ]  ,\n",
        "                                    'alpha' :params_fault_t[: , 10 ] , 'flap' : params_fault_t[: , 11 ] }  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    concat_2_df_health_fault = pd.concat([df_params , df_params_fault , df_params_fault_t ] , axis = 0 )\n",
        "    df_corr = concat_2_df_health_fault.corr()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # --- learning\n",
        "    #plt.figure(figsize= (10 , 10 ) )\n",
        "    #ax = sns.heatmap(df_corr , annot=True )\n",
        "    #print(df_corr)\n",
        "    #plt.show()\n",
        "    total_simulation['alpha'].append(iter_alpha)\n",
        "    total_simulation['flap'].append(iter_flat)\n",
        "    total_simulation['bias_correlation'].append(df_corr['class']['bias'])\n",
        "    total_simulation['betha_0_correlation'].append(df_corr['class']['betha_0'])\n",
        "    total_simulation['betha_1_correlation'].append(df_corr['class']['betha_1'])\n",
        "    total_simulation['betha_2_correlation'].append(df_corr['class']['betha_2'])\n",
        "    total_simulation['betha_3_correlation'].append(df_corr['class']['betha_3'])\n",
        "    total_simulation['betha_4_correlation'].append(df_corr['class']['betha_4'])\n",
        "    total_simulation['betha_5_correlation'].append(df_corr['class']['betha_5'])\n",
        "\n",
        "\n",
        "\n",
        "    #df2_health  =  concat_2_df_health_fault[concat_2_df_health_fault['class'] == 0 ]\n",
        "    #df2_fault   =  concat_2_df_health_fault[concat_2_df_health_fault['class'] == 1 ]\n",
        "    #df2_fault_t =  concat_2_df_health_fault[concat_2_df_health_fault['class'] == 2 ]\n",
        "\n",
        "\n",
        "    stander_u = StandardScaler()\n",
        "    stand_df_with_out = concat_2_df_health_fault.drop(['class' , 'alpha' , 'flap'] , axis = 1 )\n",
        "    stand_df_with_out = pd.DataFrame(stander_u.fit_transform(stand_df_with_out )  , columns = stand_df_with_out.columns )\n",
        "    x_u_train , x_u_test , y_u_train , y_u_test = train_test_split(stand_df_with_out , concat_2_df_health_fault['class'] , random_state=42 , test_size=0.2  )\n",
        "    obj_gaunb = GaussianNB()\n",
        "    obj_gaunb.fit(x_u_train , y_u_train )\n",
        "    pre_nb = obj_gaunb.predict(x_u_test)\n",
        "    accuray_nav = accuracy_score(pre_nb ,y_u_test )\n",
        "    print(f\"accuracy Nave Bays : {accuray_nav}\")\n",
        "    total_simulation['accuracy_nav'].append(accuray_nav)\n",
        "    classifier_1 = RandomForestClassifier()\n",
        "    classifier_1.fit(x_u_train , y_u_train )\n",
        "    pre_random = classifier_1.predict(x_u_test)\n",
        "    accuracy_random_forest = accuracy_score(pre_random ,y_u_test )\n",
        "    print(f\"accuracy accuracy_random_forest  : {accuracy_random_forest}\")\n",
        "    total_simulation['accuracy_random_forest'].append(accuracy_random_forest)\n",
        "\n",
        "    extra_classifier = ExtraTreesClassifier()\n",
        "    extra_classifier.fit(x_u_train , y_u_train )\n",
        "    predict_extra = extra_classifier.predict(x_u_test)\n",
        "    total_simulation['accuracy_extra_classifier'].append(accuracy_score(predict_extra ,y_u_test ) )\n",
        "\n",
        "    new_y_u_train_cat = to_categorical(y_u_train)\n",
        "    new_y_u_test_cat  = to_categorical(y_u_test)\n",
        "\n",
        "    model_learning = Sequential([\n",
        "        Dense(32 , activation='relu' , input_shape = x_u_train.shape[1 : ] ) ,\n",
        "        Dropout(0.2) ,\n",
        "        Dense(10 , activation= 'relu' ) ,\n",
        "        Dense(3 , activation='softmax' ) ,\n",
        "    ])\n",
        "\n",
        "    model_learning.compile('adam' , loss = 'categorical_crossentropy' , metrics = ['acc' ] )\n",
        "    model_learning_info = model_learning.fit(x_u_train , new_y_u_train_cat , epochs = 50 , validation_data = [x_u_test , new_y_u_test_cat ] )\n",
        "    plt.plot(range(50) , model_learning_info.history['acc'] , label='acc' )\n",
        "    plt.plot(range(50) , model_learning_info.history['val_acc'] , label='val_acc' )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(model_learning.evaluate(x_u_test , new_y_u_test_cat) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\" ----------------- //////// End This Iter alpha and Flap //////// ------------------------- \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if iter_alpha > 0.001 :\n",
        "    break"
      ],
      "metadata": {
        "id": "_ffAmMAWlNph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXZjK6E7lNr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}